{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1ee6820e-3961-849c-e85b-166a482af606",
    "_uuid": "81c5b2c1a6175dc61d52769c91921fd391d8ca42"
   },
   "source": [
    "# Titanic Competition from Kaggle\n",
    "\n",
    "The \"Titanic: Machine Learning from Disaster\" is a good competition to get started with ML hands-on. So, for beginners in ML I highly recommend it a try.\n",
    "\n",
    "I created this code using python to predict the survival labels for the test set in this competition. The highest score I got was 0.77990 for the accuracy of the model. In the following paragraphs I will present the steps I went through to get this score.\n",
    "\n",
    "**Note:** Keep in mind that this tutorial is just as a simple starting point and will be useful for beginners. Many more explorations and optimizations could be done. If you have any comments about this tutorial please let me know. \n",
    "\n",
    "In this tutorial I will present basic steps of a data science pipeline:\n",
    "\n",
    "#### 1. Data exploration and visualization  \n",
    "   - Explore dataset\n",
    "   - Choose important features and visualize them according to survival/non-survival\n",
    "   \n",
    "#### 2. Data cleaning, Feature selection and Feature engineering\n",
    "   - Null values\n",
    "   - Encode categorical data\n",
    "   - Transform features\n",
    "   \n",
    "#### 3. Test different classifiers \n",
    "   - Logistic Regression (LR)\n",
    "   - K-NN\n",
    "   - Support Vector Machines (SVM)\n",
    "   - Naive Bayes\n",
    "   - Random Forest (RF)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e077f539-ad91-7277-41d9-1f1246092189",
    "_uuid": "27b81e09132fb09511a5fcf7d925b9fb766e9cc3"
   },
   "source": [
    "First let's start by importing the essential libraries to work with dataframes (**pandas**), numeric values (**numpy**) and visualization (**matplotlib.pyplot**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "2dfc55af-0bc1-cae3-14b1-b16ccbaa9a16",
    "_uuid": "1c2a278c46e7e31c5b8c2c49b4240aba95e533bb"
   },
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8c4c9f57-5fb2-4037-6b92-ff3ffa6f87c8",
    "_uuid": "2a30885a2dc5487417bf2e7d6cb45311476f9118"
   },
   "source": [
    "Now let's import the csv file with the training dataset. You can download it from [here](https://www.kaggle.com/c/titanic/data).  The explanation of the features (each column from the dataset) is also presented in this link. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a2bacde5-f0d4-566c-ffdc-512c64ddb9f5",
    "_uuid": "f8f6b85462fb3b911130b1b3fc5dc6a193a948fb"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File ../input/train.csv does not exist: '../input/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cea6879f2839>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../input/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ml\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File ../input/train.csv does not exist: '../input/train.csv'"
     ]
    }
   ],
   "source": [
    "dataset= pd.read_csv(\"../input/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "798fb3c7-7e83-1410-846e-89d1c673c233",
    "_uuid": "1adf1c8609f3a7dd48e8c7472ec192faebd11759"
   },
   "source": [
    "## 1. Data exploration and visualization  \n",
    "\n",
    "For a good start, we should look at the dataset. Analyze the features and think which could be useful to predict the survival rate. The features that probably may have an influence are: the **\"P-class\"** (expect to see more survival for higher class), the **\"Sex\"** and **\"Age\"** (\"women and children first\"), and let's say **\"Embarked\"** also. \n",
    "\n",
    "We will now plot some graphs to confirm if these features show some relation with the survival rate. These plots were based in the graphs presented [here](http://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a51d93fc-1e64-f21e-0890-0554ed365f2e",
    "_uuid": "203aab99bc20b879a65b70addd4b75e553cf3e60",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import seaborn\n",
    "seaborn.set() \n",
    "\n",
    "#-------------------Survived/Died by Class -------------------------------------\n",
    "survived_class = dataset[dataset['Survived']==1]['Pclass'].value_counts()\n",
    "dead_class = dataset[dataset['Survived']==0]['Pclass'].value_counts()\n",
    "df_class = pd.DataFrame([survived_class,dead_class])\n",
    "df_class.index = ['Survived','Died']\n",
    "df_class.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived/Died by Class\")\n",
    "\n",
    "Class1_survived= df_class.iloc[0,0]/df_class.iloc[:,0].sum()*100\n",
    "Class2_survived = df_class.iloc[0,1]/df_class.iloc[:,1].sum()*100\n",
    "Class3_survived = df_class.iloc[0,2]/df_class.iloc[:,2].sum()*100\n",
    "print(\"Percentage of Class 1 that survived:\" ,round(Class1_survived),\"%\")\n",
    "print(\"Percentage of Class 2 that survived:\" ,round(Class2_survived), \"%\")\n",
    "print(\"Percentage of Class 3 that survived:\" ,round(Class3_survived), \"%\")\n",
    "\n",
    "# display table\n",
    "from IPython.display import display\n",
    "display(df_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "cea9edcb-cedc-2d41-07b0-5dcc86ddf6ef",
    "_uuid": "86dd1ee1a8e3b9b09eaa437dc144e14691c418b3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------Survived/Died by SEX------------------------------------\n",
    "   \n",
    "Survived = dataset[dataset.Survived == 1]['Sex'].value_counts()\n",
    "Died = dataset[dataset.Survived == 0]['Sex'].value_counts()\n",
    "df_sex = pd.DataFrame([Survived , Died])\n",
    "df_sex.index = ['Survived','Died']\n",
    "df_sex.plot(kind='bar',stacked=True, figsize=(5,3), title=\"Survived/Died by Sex\")\n",
    "\n",
    "\n",
    "female_survived= df_sex.female[0]/df_sex.female.sum()*100\n",
    "male_survived = df_sex.male[0]/df_sex.male.sum()*100\n",
    "print(\"Percentage of female that survived:\" ,round(female_survived), \"%\")\n",
    "print(\"Percentage of male that survived:\" ,round(male_survived), \"%\")\n",
    "\n",
    "# display table\n",
    "from IPython.display import display\n",
    "display(df_sex) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d6b077b8-4d61-e44a-84cd-d03cb9b3d9f6",
    "_uuid": "23470600cafce7e3e3792df2b04ca141b425bd0a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------- Survived/Died by Embarked ----------------------------\n",
    "\n",
    "survived_embark = dataset[dataset['Survived']==1]['Embarked'].value_counts()\n",
    "dead_embark = dataset[dataset['Survived']==0]['Embarked'].value_counts()\n",
    "df_embark = pd.DataFrame([survived_embark,dead_embark])\n",
    "df_embark.index = ['Survived','Died']\n",
    "df_embark.plot(kind='bar',stacked=True, figsize=(5,3))\n",
    "\n",
    "Embark_S= df_embark.iloc[0,0]/df_embark.iloc[:,0].sum()*100\n",
    "Embark_C = df_embark.iloc[0,1]/df_embark.iloc[:,1].sum()*100\n",
    "Embark_Q = df_embark.iloc[0,2]/df_embark.iloc[:,2].sum()*100\n",
    "print(\"Percentage of Embark S that survived:\", round(Embark_S), \"%\")\n",
    "print(\"Percentage of Embark C that survived:\" ,round(Embark_C), \"%\")\n",
    "print(\"Percentage of Embark Q that survived:\" ,round(Embark_Q), \"%\")\n",
    "\n",
    "from IPython.display import display\n",
    "display(df_embark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2dc5765e-a510-dd4c-7ba1-0db6b19877bb",
    "_uuid": "38b39ed6d252da21a2e0c0b5318b0f01ec43892f"
   },
   "source": [
    "## 2. Data cleaning, Feature selection and Feature engineering\n",
    "The preprocessing of the data is a quite crucial part. If we just give the dataset without cleaning it, most probably the results will not be good! So, in this step we will preprocess the training dataset and this will involve feature selection, data cleaning, and feature engineering.   \n",
    "\n",
    "I will start with feature selection. As we saw previously, **\"P-Class\", \"Sex\", \"Age\"** and **\"Embarked\"** showed some relation with Survived rate. Thus, I will drop the remaining features, except **\"Name\"** because it will be useful in a further step of the cleaning process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "387d5d13-49f2-1d66-38c4-34e1df20c121",
    "_uuid": "71cb93cb7c5e5888493ce482ed215e61a14da46c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dataset.drop(['PassengerId','Cabin','Ticket','Fare', 'Parch', 'SibSp'], axis=1)\n",
    "y = X.Survived                       # vector of labels (dependent variable)\n",
    "X=X.drop(['Survived'], axis=1)       # remove the dependent variable from the dataframe X\n",
    "\n",
    "X.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bc7fbaba-9357-332d-2621-beb5237931b3",
    "_uuid": "6767b71c457277ae1dd32a0a16fd24af63c2f39e"
   },
   "source": [
    "We can see, from this displayed DataFrame, that **\"Sex\"** and **\"Embarked\"** are categorical features and have strings instead of numeric values. We need to encode these strings into numeric data, so the algorithm can perform its calculations. \n",
    "\n",
    "For the **\"Sex\"** feature we can use the **LabelEncoder** class from  **sklearn.preprocessing** library. \n",
    "\n",
    "Another way of doing this is by using the **get_dummies** from **pandas**. We will be using this to encode the **\"Embarked\"** feature. But first, as **\"Embarked\"** has two NaN values we need to take care of these missing values. In this approach, I will provide the 'S' category because it is the most frequent in the data. After this, it is then possible to use the **get_dummies** and get three new columns (Embarked_C,\tEmbarked_Q, Embarked_S) which are called dummy variables (they assign ‘0’ and ‘1’ to indicate membership in a category). The previous **\"Embarked\"** can be dropped from X as it will not be needed anymore and we can now concatenate the X dataframe with the new **\"Embarked\"** which has the three dummy variables. Finally, as the number of dummy variables necessary to represent a single feature is equal to the number of categories in that feature minus one, we can remove one of the dummies created, lets say Embarked_S, for example. This will not remove any information because by having the values from Embarked_C and\tEmbarked_Q the algorithm can easily understand the values from the remaining dummy variable (when Embarked_C and Embarked_Q are '0' Embarked_S will be '1', otherwise it will be '0').  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "07df4aa0-b658-32e7-e781-2c26a2dd187b",
    "_uuid": "ae13c79c315b889f243f5f7f68d76aee58951cc1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------------- Encoding categorical data -------------------------\n",
    "\n",
    "# encode \"Sex\"\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelEncoder_X = LabelEncoder()\n",
    "X.Sex=labelEncoder_X.fit_transform(X.Sex)\n",
    "\n",
    "\n",
    "# encode \"Embarked\"\n",
    "\n",
    "# number of null values in embarked:\n",
    "print ('Number of null values in Embarked:', sum(X.Embarked.isnull()))\n",
    "\n",
    "# fill the two values with one of the options (S, C or Q)\n",
    "row_index = X.Embarked.isnull()\n",
    "X.loc[row_index,'Embarked']='S' \n",
    "\n",
    "Embarked  = pd.get_dummies(  X.Embarked , prefix='Embarked'  )\n",
    "X = X.drop(['Embarked'], axis=1)\n",
    "X= pd.concat([X, Embarked], axis=1)  \n",
    "# we should drop one of the columns\n",
    "X = X.drop(['Embarked_S'], axis=1)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "6b654220-d347-88f2-3c2e-8159aa9cd887",
    "_uuid": "18f27fc30dc0b0d888529513b84ada89f0fbf1d9"
   },
   "source": [
    "You may wonder why are we still keeping the **\"Name\"** column. In fact the name does not seem to have influence, it does not matter if a person is named Owen or William, however this column has the title located after the Surname and the comma (\"Mr\", \"Mrs\", \"Miss\", etc.) which can be useful.  \n",
    "\n",
    "If we take a look at the table X displayed previously we can see many missing values for the **\"Age\"** column. Removing these rows with missing values would involve removing 177 rows (which is quite a lot!) and we would have less information to create the model. In some cases, it is acceptable to take the average of the column and replace the null values, nonetheless in this case, it is possible to estimate the age of the person by their title, present in the **\"Name\"** column.   \n",
    "\n",
    "Therefore, I will first identify the different titles presented and then average the Age for each title. We can provide this averaged Age found for each title to the people with missing Age values, accordingly to their title in **\"Name\"**. \n",
    "\n",
    "After using the information in **\"Name\"** we can drop this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "fdbf5bc4-1daf-2f04-4480-6889f0dd5994",
    "_uuid": "7347993a92c38be615e254400f758a423615a203",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------- Taking care of missing data  -----------------------------\n",
    "\n",
    "print ('Number of null values in Age:', sum(X.Age.isnull()))\n",
    " \n",
    "\n",
    "# -------- Change Name -> Title ----------------------------\n",
    "got= dataset.Name.str.split(',').str[1]\n",
    "X.iloc[:,1]=pd.DataFrame(got).Name.str.split('\\s+').str[1]\n",
    "# ---------------------------------------------------------- \n",
    "\n",
    "\n",
    "#------------------ Average Age per title -------------------------------------------------------------\n",
    "ax = plt.subplot()\n",
    "ax.set_ylabel('Average age')\n",
    "X.groupby('Name').mean()['Age'].plot(kind='bar',figsize=(13,8), ax = ax)\n",
    "\n",
    "title_mean_age=[]\n",
    "title_mean_age.append(list(set(X.Name)))  #set for unique values of the title, and transform into list\n",
    "title_mean_age.append(X.groupby('Name').Age.mean())\n",
    "title_mean_age\n",
    "#------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "#------------------ Fill the missing Ages ---------------------------\n",
    "n_traning= dataset.shape[0]   #number of rows\n",
    "n_titles= len(title_mean_age[1])\n",
    "for i in range(0, n_traning):\n",
    "    if np.isnan(X.Age[i])==True:\n",
    "        for j in range(0, n_titles):\n",
    "            if X.Name[i] == title_mean_age[0][j]:\n",
    "                X.Age[i] = title_mean_age[1][j]\n",
    "#--------------------------------------------------------------------    \n",
    "\n",
    "X=X.drop(['Name'], axis=1)\n",
    "\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "ba200ead-be7b-a2d8-a952-a86784bb2fb4",
    "_uuid": "39b9d429854733ad4c3bffd1812c33d180563e40"
   },
   "source": [
    "We can also make feature transformation. For example, we could transform the **\"Age\"** feature in order to simplify it. We could distinguish the youngsters (age less than 18 years) from the adults.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "51107085-0fcc-d28f-8de8-fa7e2b2379c6",
    "_uuid": "d8ea3676b8ef721bef3e9d5990fdc1bd466f878f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, n_traning):\n",
    "    if X.Age[i] > 18:\n",
    "        X.Age[i]= 0\n",
    "    else:\n",
    "        X.Age[i]= 1\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "40369ea6-f78a-a8b6-3391-6e0c79de4c07",
    "_uuid": "76ac44a302d9ff9851685e9af40133b8d63647fe"
   },
   "source": [
    "Now, we can say that we have a quite well clean dataset to provide to our classifier algorithm. \n",
    "\n",
    "\n",
    "## 3. Test different classifiers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5209a5b3-cdbe-911e-5c71-9a77594f8859",
    "_uuid": "56e33c9970a8eef88cd8cc8f8aaf7a0f2d054de6"
   },
   "source": [
    "Having the data preprocessed we can now provide the data to different classifiers and see which one performs better in creating a model of classification for this data. \n",
    "\n",
    "We will use cross validation, which is a model validation technique to evaluate how well a model will generalize to an independent data set. Python has the **cross_val_score** class from **sklearn.model_selection** library to perform cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e0a7684c-c3a2-f223-7e88-278d1b8fef65",
    "_uuid": "650a840160f016e5faff3dde56728b1a7cb204f3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-----------------------Logistic Regression---------------------------------------------\n",
    "# Fitting Logistic Regression to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression(penalty='l2',random_state = 0)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"Logistic Regression:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#-----------------------------------K-NN --------------------------------------------------\n",
    "\n",
    "# Fitting K-NN to the Training set\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 9, metric = 'minkowski', p = 2)\n",
    "\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"K-NN:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n",
    "\n",
    "\n",
    "#---------------------------------------SVM -------------------------------------------------\n",
    "\n",
    "# Fitting Kernel SVM to the Training set\n",
    "from sklearn.svm import SVC\n",
    "classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"SVM:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n",
    "\n",
    "\n",
    "#---------------------------------Naive Bayes-------------------------------------------\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"Naive Bayes:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std(),\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "#----------------------------Random Forest------------------------------------------\n",
    "\n",
    "# Fitting Random Forest Classification to the Training set\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\n",
    "\n",
    "# Applying k-Fold Cross Validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator = classifier, X=X , y=y , cv = 10)\n",
    "print(\"Random Forest:\\n Accuracy:\", accuracies.mean(), \"+/-\", accuracies.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d321bf36-7b32-7afd-2eda-55d68001e66b",
    "_uuid": "4df3700f7514f86ed7e987da5ea976ad81cb1153"
   },
   "source": [
    "As we can see, from all the 5 classifiers tested in this tutorial, **Random Forest** got better results. \n",
    "\n",
    "After changing the test set by performing the same transformations done in the training set we can then use the **Random Forest** model created and do the predictions. The submission of these predictions was scored 0.77990 in Kaggle.  \n",
    "\n",
    "Hope this tutorial was useful in some way. For a more detailed tutorial in the Titanic challenge I recommend this [tutorial](http://ahmedbesbes.com/how-to-score-08134-in-titanic-kaggle-challenge.html).  "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
